{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b688587-4ae4-43be-be8f-1e19674194b7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34b9ed4-8113-4d35-abd4-2eea709b3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from datetime import datetime\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891d3ab-dc43-4d61-b9b8-3130b31f5fea",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8cfce8-069a-4b52-9606-673264ae13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.5   # content similarity weight\n",
    "BETA  = 0.3   # collaborative score weight\n",
    "GAMMA = 0.1   # interest overlap\n",
    "DELTA = 0.05  # recency boost\n",
    "EPS   = 0.05  # popularity weight\n",
    "N_COMPONENTS_SVD = 50\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce91c9-ae4c-42bf-8f7c-400cbf2bb8e7",
   "metadata": {},
   "source": [
    "# Loading CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bacf67-2ad1-4ea1-8748-eeeaf70b70b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users (50, 5)\n",
      "posts (100, 4)\n",
      "engagements (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv('datasets/Users.csv')\n",
    "posts = pd.read_csv('datasets/Posts.csv')\n",
    "eng = pd.read_csv('datasets/Engagements.csv')\n",
    "\n",
    "# Quick glance\n",
    "print(\"users\", users.shape)\n",
    "print(\"posts\", posts.shape)\n",
    "print(\"engagements\", eng.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb31102-7896-4113-a7d4-2d9188066336",
   "metadata": {},
   "source": [
    "# Building textual field for posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dfb1337-3587-4885-9e84-722a2fdbf1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_post_text(row):\n",
    "    parts = []\n",
    "    for c in ['title', 'body', 'tags']:\n",
    "        if c in posts.columns and pd.notna(row.get(c)):\n",
    "            parts.append(str(row.get(c)))\n",
    "    return ' '.join(parts)\n",
    "\n",
    "posts['text'] = posts.apply(combine_post_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc5e0d-b1bc-4e15-96cc-5069ae69954e",
   "metadata": {},
   "source": [
    "# TF-IDF for posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4103e8f8-d890-4815-b5b3-e12aa9a7958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "post_tfidf = tfidf.fit_transform(posts['text'].fillna(''))\n",
    "# reduce dim for speed / denoise\n",
    "svd = TruncatedSVD(n_components=min(N_COMPONENTS_SVD, post_tfidf.shape[1]-1), random_state=42)\n",
    "post_emb = svd.fit_transform(post_tfidf)\n",
    "post_emb = normalize(post_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a092a3-47d5-4d2c-ad99-dc2cd306072a",
   "metadata": {},
   "source": [
    "# Building user interest vectors from profile interests (TF-IDF transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac35aae6-ad27-4e23-b74e-e95e88efd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'interests' in users.columns:\n",
    "    user_interest_text = users['top_3_interests'].fillna('').astype(str)\n",
    "    user_tfidf = tfidf.transform(user_interest_text)  # using same vectorizer\n",
    "    user_emb = svd.transform(user_tfidf)\n",
    "    user_emb = normalize(user_emb)\n",
    "else:\n",
    "    # fallback: average embeddings of posts the user engaged with (cold start handled later)\n",
    "    user_emb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639c01c-d5a5-4f0d-842b-2a08a05273c5",
   "metadata": {},
   "source": [
    "# Collaborative matrix\n",
    "Converting engagement types to weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3aeea90-f159-4598-b3e0-8c3ae2f4ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_weight = {'view': 1, 'click': 1, 'like': 3, 'comment': 5, 'share': 4}\n",
    "eng['weight'] = eng['engagement'].astype(float)\n",
    "\n",
    "# Building user-item sparse matrix\n",
    "user_ids = eng['user_id'].unique()\n",
    "post_ids = posts['post_id'].unique()\n",
    "\n",
    "# Maping to indices\n",
    "user2idx = {u:i for i,u in enumerate(eng['user_id'].unique())}\n",
    "post2idx = {p:i for i,p in enumerate(posts['post_id'].unique())}\n",
    "\n",
    "n_users = len(user2idx)\n",
    "n_posts = len(post2idx)\n",
    "\n",
    "# Building dense matrix (if large, switch to sparse)\n",
    "ui = np.zeros((n_users, n_posts), dtype=float)\n",
    "for _, r in eng.iterrows():\n",
    "    u = r['user_id']; p = r['post_id']; w = r['weight']\n",
    "    if u in user2idx and p in post2idx:\n",
    "        ui[user2idx[u], post2idx[p]] += w\n",
    "\n",
    "# Normalising rows\n",
    "ui_norm = normalize(ui, norm='l2', axis=1)\n",
    "# Computing low-rank SVD on user-item matrix to get latent factors\n",
    "svd_ui = TruncatedSVD(n_components=min(50, min(ui_norm.shape)-1), random_state=42)\n",
    "ui_factors = svd_ui.fit_transform(ui_norm)  # user latent\n",
    "item_factors = svd_ui.components_.T        # post latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134f2c7-80a2-41aa-9bea-a0c618c03860",
   "metadata": {},
   "source": [
    "# Helper scoring components\n",
    "content_sim: cosine between user_emb and post_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f51ecdce-37e1-4326-9893-2e81410e3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_sim_for_user(user_idx):\n",
    "    uvec = user_emb[user_idx] if user_emb is not None else None\n",
    "    if uvec is None:\n",
    "        return np.zeros(n_posts)\n",
    "    sims = cosine_similarity(uvec.reshape(1, -1), post_emb).ravel()\n",
    "    return sims\n",
    "\n",
    "# cf score via dot product of user latent and post latent\n",
    "def cf_score_for_user(user_id):\n",
    "    if user_id not in user2idx:\n",
    "        return np.zeros(n_posts)\n",
    "    uidx = user2idx[user_id]\n",
    "    user_lat = ui_factors[uidx]\n",
    "    scores = item_factors.dot(user_lat)\n",
    "    # optionally normalize\n",
    "    return (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
    "\n",
    "# interest overlap: Jaccard between user interest keywords and post tags\n",
    "def interest_overlap(user_row, posts_df):\n",
    "    if 'interests' not in user_row or 'tags' not in posts_df.columns:\n",
    "        return np.zeros(len(posts_df))\n",
    "    uset = set(str(user_row['interests']).lower().split(','))\n",
    "    overlaps = []\n",
    "    for tags in posts_df['tags'].fillna(''):\n",
    "        pset = set(str(tags).lower().split(','))\n",
    "        if len(uset)==0 and len(pset)==0:\n",
    "            overlaps.append(0.0)\n",
    "        else:\n",
    "            overlaps.append(len(uset & pset)/ (len(uset | pset) + 1e-9))\n",
    "    return np.array(overlaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dfc22-c2d3-442d-a4ed-153fc08a6ec4",
   "metadata": {},
   "source": [
    "# Recency and popularity\n",
    "post created_at assumed ISO or unix; computing age in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ef47c3-9e72-4600-afd6-8653d7c4a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'created_at' in posts.columns:\n",
    "    posts['created_at'] = pd.to_datetime(posts['created_at'], errors='coerce')\n",
    "else:\n",
    "    posts['created_at'] = pd.NaT\n",
    "today = pd.Timestamp.now()\n",
    "posts['age_days'] = (today - posts['created_at']).dt.days.fillna(9999)\n",
    "# recency boost: exp decay\n",
    "posts['recency_boost'] = np.exp(-posts['age_days'] / 30.0)  # half life ~20-30 days\n",
    "\n",
    "# popularity: total engagement count normalized\n",
    "post_eng_counts = eng.groupby('post_id')['weight'].sum().reindex(posts['post_id']).fillna(0).values\n",
    "posts['popularity'] = (post_eng_counts - post_eng_counts.min()) / (post_eng_counts.max() - post_eng_counts.min() + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8af2bf-73db-4088-bb6a-5a67dc6389e8",
   "metadata": {},
   "source": [
    "# Full scoring and recommendation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d169fe-c3eb-49a7-a402-be2049a0aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(user_row, user_id):\n",
    "    # mapping to embedding index: for user_emb we used users order; ensure alignment\n",
    "    # finding user index in users DataFrame\n",
    "    try:\n",
    "        uidx_df = users.index[users['user_id']==user_id][0]\n",
    "    except Exception:\n",
    "        uidx_df = None\n",
    "\n",
    "    # content similarity\n",
    "    content_scores = np.zeros(n_posts)\n",
    "    if uidx_df is not None and user_emb is not None:\n",
    "        content_scores = cosine_similarity(user_emb[uidx_df].reshape(1,-1), post_emb).ravel()\n",
    "\n",
    "    # cf score\n",
    "    cf_scores = cf_score_for_user(user_id)\n",
    "\n",
    "    # interest overlap\n",
    "    interest_scores = interest_overlap(user_row, posts)\n",
    "\n",
    "    # recency and popularity\n",
    "    recency = posts['recency_boost'].values\n",
    "    popularity = posts['popularity'].values\n",
    "\n",
    "    # combining\n",
    "    score = ALPHA*content_scores + BETA*cf_scores + GAMMA*interest_scores + DELTA*recency + EPS*popularity\n",
    "\n",
    "    # mask posts the user already engaged with if desired (to recommend new posts)\n",
    "    if user_id in user2idx:\n",
    "        engaged_post_idxs = [post2idx[p] for p in eng[eng['user_id']==user_id]['post_id'].unique() if p in post2idx]\n",
    "        # optional: deprioritize already engaged\n",
    "        score[engaged_post_idxs] *= 0.5\n",
    "\n",
    "    top_idx = np.argsort(-score)[:TOP_K]\n",
    "    top_posts = posts.iloc[[list(post2idx.keys()).index(posts['post_id'].iloc[i]) if False else i for i in top_idx]]\n",
    "    # easier: use post_id via mapping from index numbers\n",
    "    recommended_post_ids = posts.iloc[top_idx]['post_id'].tolist()\n",
    "    return recommended_post_ids, score[top_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930d8ab-c488-45d5-b730-3bad710067be",
   "metadata": {},
   "source": [
    "# Producing recommendations for all users and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022d4a22-1a35-4f4c-9d5b-6749bd87ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = []\n",
    "for _, urow in users.iterrows():\n",
    "    user_id = urow['user_id']\n",
    "    top_posts, scores = recommend_for_user(urow, user_id)\n",
    "    recs.append({\n",
    "        'user_id': user_id,\n",
    "        'rec_1': top_posts[0] if len(top_posts)>0 else None,\n",
    "        'rec_2': top_posts[1] if len(top_posts)>1 else None,\n",
    "        'rec_3': top_posts[2] if len(top_posts)>2 else None,\n",
    "    })\n",
    "recs_df = pd.DataFrame(recs)\n",
    "recs_df.head()\n",
    "# Saving to CSV\n",
    "recs_df.to_csv('recommendations_top3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d2e214-840c-4ebb-8ad3-f9d42c4759c1",
   "metadata": {},
   "source": [
    "# Evaluation helpers\n",
    "Defining Precision@k, nDCG@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e41298c-c7f7-4e59-bb2c-a95094b1da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended, actual, k=3):\n",
    "    if len(recommended)==0:\n",
    "        return 0.0\n",
    "    return len(set(recommended[:k]) & set(actual)) / k\n",
    "\n",
    "def dcg_at_k(recommended, actual, k):\n",
    "    dcg = 0.0\n",
    "    for i, r in enumerate(recommended[:k]):\n",
    "        rel = 1.0 if r in actual else 0.0\n",
    "        denom = math.log2(i+2)  # i+2 because i=0 => position 1\n",
    "        dcg += rel/denom\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(recommended, actual, k):\n",
    "    idcg = sum([1.0/math.log2(i+2) for i in range(min(len(actual), k))])\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(recommended, actual, k)/idcg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb8650-d5c8-4231-be30-fe38645bfb68",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Title: **Interest-based Content Recommendation — Approach & Evaluation**\n",
    "\n",
    "1. **Problem statement**  \n",
    "Recommend top-3 posts for each user using profile interests, past engagement, and post attributes.\n",
    "\n",
    "2. **Data & assumptions**  \n",
    "- `Users.csv`: `user_id`, `age`, `gender`, `top_3_interests`, `past_engagement_score`  \n",
    "- `Posts.csv`: `post_id`, `creater_it`, `content_type`, `tags`  \n",
    "- `Engagements.csv`: `user_id`, `post_id`, `engagement`  \n",
    "\n",
    "3. **Approach**\n",
    "- **Content modeling:** TF-IDF on post text (title/body/tags), reduced via TruncatedSVD → normalized embeddings.\n",
    "- **Profile matching:** transform user interests through same TF-IDF+SVD pipeline to get user content vectors; compute cosine similarity against posts.\n",
    "- **Collaborative signal:** build implicit user–item matrix with weighted engagements, normalize and compute low-rank SVD to obtain user/item latent factors; score by latent dot product.\n",
    "- **Heuristics:** recency (exponential decay), popularity (total weighted engagements normalized), deprioritize already engaged posts optionally.\n",
    "- **Ensemble scoring:** linear combination with tunable weights α..ε optimized on validation set.\n",
    "\n",
    "4. **Evaluation**\n",
    "- Metrics: **Precision@3**, **Recall@3**, **nDCG@3**, **MAP**.  \n",
    "- Validation: Time-aware split (train on engagements up to T, validate on engagements after T) to mimic production.\n",
    "\n",
    "5. **Extensions**\n",
    "- Use pretrained sentence embeddings (SBERT / Universal Sentence Encoder) for better semantic matches.  \n",
    "- Train a learning-to-rank model (e.g., LightGBM ranker or pairwise LambdaMART) with engineered features.  \n",
    "- Add sequential models (RNN/Transformer) for session-based recency.  \n",
    "- Use LightFM or implicit ALS for more robust collaborative filtering.  \n",
    "- Online A/B testing, logged-in/offline evaluation and calibration.\n",
    "\n",
    "6. **Notes on fairness & safety**\n",
    "- Provide controls to avoid filter bubbles: introduce serendipity / diversify recommendations by content type or topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
